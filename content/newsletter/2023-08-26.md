---
title: "2023 08 26"
date: 2023-08-26T08:16:21Z
# weight: 1
# aliases: ["/first"]
tags: ["archive"]
author: "blackjack"
# author: ["Me", "You"] # multiple authors
showToc: true
TocOpen: false
draft: false
hidemeta: false
comments: false
description: "Desc Text."
canonicalURL: "https://canonical.url/to/page"
disableHLJS: true # to disable highlightjs
disableShare: false
disableHLJS: false
hideSummary: false
searchHidden: true
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: true
ShowRssButtonInSectionTermList: true
UseHugoToc: true
cover:
    image: "<image path/url>" # image path/url
    alt: "<alt text>" # alt text
    caption: "<text>" # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: true # only hide on current single page
editPost:
    URL: "https://github.com/yifu-yang/yifu-yang.github.io/content"
    Text: "Suggest Changes" # edit text
    appendFilePath: true # to append file path to Edit link
---
## 1. Beating GPT-4 on HumanEval with a fine-tuned CodeLlama-34B

rushingcreek:Hi HN,<p>We have fine-tuned CodeLlama-34B and CodeLlama-34B-Python on an internal Phind dataset that achieved 67.6% and 69.5% pass@1 on HumanEval, respectively. GPT-4 achieved 67%. To ensure result validity, we applied OpenAI&#x27;s decontamination methodology to our dataset.<p>The CodeLlama models released yesterday demonstrate impressive performance on HumanEval.<p>- CodeLlama-34B achieved 48.8% pass@1 on HumanEval<p>- CodeLlama-34B-Python achieved 53.7% pass@1 on HumanEval<p>We have fine-tuned both models on a proprietary dataset of ~80k high-quality programming problems and solutions. Instead of code completion examples, this dataset features instruction-answer pairs, setting it apart structurally from HumanEval. We trained the Phind models over two epochs, for a total of ~160k examples. LoRA was not used — both models underwent a native fine-tuning. We employed DeepSpeed ZeRO 3 and Flash Attention 2 to train these models in three hours using 32 A100-80GB GPUs, with a sequence length of 4096 tokens.<p>Furthermore, we applied OpenAI&#x27;s decontamination methodology to our dataset to ensure valid results, and found no contaminated examples.<p>The methodology is:<p>- For each evaluation example, we randomly sampled three substrings of 50 characters or used the entire example if it was fewer than 50 characters.<p>- A match was identified if any sampled substring was a substring of the processed training example.<p>For further insights on the decontamination methodology, please refer to Appendix C of OpenAI&#x27;s technical report.<p>Presented below are the pass@1 scores we achieved with our fine-tuned models:<p>- Phind-CodeLlama-34B-v1 achieved 67.6% pass@1 on HumanEval<p>- Phind-CodeLlama-34B-Python-v1 achieved 69.5% pass@1 on HumanEval<p>Note on GPT-4<p>According to the official technical report in March, OpenAI reported a pass@1 score of 67% for GPT-4&#x27;s performance on HumanEval. Since then, there have been claims reporting higher scores. However, it&#x27;s essential to note that there hasn&#x27;t been any concrete evidence pointing towards an enhancement in the model&#x27;s coding abilities since then. It&#x27;s also crucial to highlight that these elevated figures lack the rigorous contamination analysis that the official statistic underwent, making them less of a reliable comparison. As a result, we consider 67% as the pass@1 score for GPT-4.<p>Download<p>We are releasing both models on Huggingface for verifiability and to bolster the open-source community. We welcome independent verification of results.<p>Phind-CodeLlama-34B-v1: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;Phind&#x2F;Phind-CodeLlama-34B-v1" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;Phind&#x2F;Phind-CodeLlama-34B-v1</a><p>Phind-CodeLlama-34B-Python-v1: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;Phind&#x2F;Phind-CodeLlama-34B-Python-v1" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;Phind&#x2F;Phind-CodeLlama-34B-Python-v1</a><p>We&#x27;d love to hear your thoughts!<p>Best,<p>The Phind Team

[原文链接](https://www.phind.com/blog/code-llama-beats-gpt4)

## 2. Deep Neural Nets: 33 years ago and 33 years from now (2022)

gsky:

[原文链接](http://karpathy.github.io/2022/03/14/lecun1989/)

## 3. Giving up the iPad-only travel dream

haunter:

[原文链接](https://sixcolors.com/post/2023/08/why-i-gave-up-on-the-ipad-only-dream/)

## 4. OpenTF announces fork of Terraform

cube2222:

[原文链接](https://opentf.org/announcement)

## 5. Programming as Theory Building (1985) [pdf]

solomonb:

[原文链接](https://algoritmos-iii.github.io/assets/bibliografia/programming-as-theory-building.pdf)

## 6. Interpretable graph neural networks for tabular data

PaulHoule:

[原文链接](https://arxiv.org/abs/2308.08945)

## 7. Prof. Robert Boice's Rules of the Road for Writers

yamrzou:

[原文链接](https://home.uchicago.edu/~rfulton/Tips.htm)

## 8. A study of research on the psychology of expertise in weather forecasting

mooreds:

[原文链接](https://mitpress.mit.edu/9780262548816/minding-the-weather/)

## 9. Gym Class VR (YC W22) is hiring a Unity animator and programmer

hackerews:

[原文链接](https://www.ycombinator.com/companies/gym-class-by-irl-studios/jobs/7UKmLED-animator-programmer-vr-unity)

## 10. Web scraping for me, but not for thee

mhb:

[原文链接](https://blog.ericgoldman.org/archives/2023/08/web-scraping-for-me-but-not-for-thee-guest-blog-post.htm)

## 11. Email Authentication: A Developer's Guide

zenorocha:

[原文链接](https://resend.com/blog/email-authentication-a-developers-guide)

## 12. FFmpeg Explorer

notbeuller:

[原文链接](https://ffmpeg.lav.io)

## 13. Research group detects a quantum entanglement wave for the first time

jdmark:

[原文链接](https://www.aalto.fi/en/news/research-group-detects-a-quantum-entanglement-wave-for-the-first-time-using-real-space-measurements)

## 14. Training immune cells to remove ‘trash’ helps resolve lung inflammation

clouddrover:

[原文链接](https://today.uic.edu/immune-cells-acute-lung-injury/)

## 15. Slime Molds [video]

oatmeal1:

[原文链接](https://www.youtube.com/watch?v=gpt9cJrEZ_Y)

## 16. I cycled to all the villages in alphabetical order

pabs3:

[原文链接](https://diziet.dreamwidth.org/16260.html)

## 17. Llama2.c L2E LLM – Multi OS Binary and Unikernel Release

AMICABoard:

[原文链接](https://github.com/trholding/llama2.c/releases/tag/L2E_v0.1)

## 18. How to Read an RFC (2018)

ademarre:

[原文链接](https://www.ietf.org/blog/how-read-rfc/)

## 19. Casio Caleid XM-700 Mobile Navigator (1997)

msephton:

[原文链接](https://blog.gingerbeardman.com/2023/08/25/casio-caleid-xm700-mobile-navigator-hardware/)

## 20. Engineering Serendipity: Does Sharing Lead to Knowledge Production? [pdf]

larve:

[原文链接](https://www.hbs.edu/ris/Publication%20Files/20-058_39f454e9-bef0-4bed-bfa8-526e90601ade.pdf)

## 21. Manuals Showcase

sohkamyung:

[原文链接](https://archive.org/details/manuals_showcase)

## 22. The EU's war on behavioral advertising

curiousbird:

[原文链接](https://thisisunpacked.substack.com/p/the-eu-war-on-behavioral-advertising)

## 23. A scalar triple product identity

luu:

[原文链接](https://realtimecollisiondetection.net/blog/?p=69)

## 24. Fifth Circuit: Law enforcement doesn’t need warrants to search phones at border

latexr:

[原文链接](https://www.techdirt.com/2023/08/25/fifth-circuit-says-law-enforcement-doesnt-need-warrants-to-search-phones-at-the-border/)

## 25. Who were the real Sin Eaters? (2021)

walterbell:

[原文链接](https://www.panmacmillan.com/blogs/fiction/megan-campisi-on-the-sin-eater)

## 26. Sidewalk Garden

zdw:

[原文链接](https://zachklein.com/Sidewalk+Garden)

## 27. BASH Stack – Bash Awk Sed HTMX web framework

elchief:

[原文链接](https://bashsta.cc/)

## 28. HelloSystem: A graphical OS built on FreeBSD

gautamcgoel:

[原文链接](https://hellosystem.github.io/docs/)

## 29. This week in KDE: Double-click by default

LorenDB:

[原文链接](https://pointieststick.com/2023/08/18/this-week-in-kde-double-click-by-default/)

## 30. We are not empty: The concept of the atomic void is a mistake

drdee:

[原文链接](https://aeon.co/essays/why-the-empty-atom-picture-misunderstands-quantum-theory)
